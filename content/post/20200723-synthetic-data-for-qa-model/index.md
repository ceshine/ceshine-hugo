---
slug: synthetic-data-for-qa-model
date: 2020-07-23T00:00:00.000Z
title: "[Notes] Training Question Answering Models From Synthetic Data"
description: "Generate questions and answers from both real and synthetic contexts"
tags:
  - nlp
  - qa
keywords:
  - transformer
  - nlp
  - bert
  - deep learning
url: /post/synthetic-data-for-qa-model/
---

{{< figure src="featuredImage.jpg" caption="[Photo Credit](https://unsplash.com/photos/Oaqk7qqNh_c)" >}}

# Preamble

[“Training Question Answering Models From Synthetic Data”](http://arxiv.org/abs/2002.09599) is a NLP paper from Nvidia that I found very interesting. Question and answer(QA) data is expansive to obtain. If we can used the data we have to generate more data, that will be a huge time saver and create a lot of new possibilities. This paper shows some promising results in this direction.

Some caveats:

1. We need models to be able to get decent results. (The paper reported that the question generation model with number of parameters from 117M to 8.3B. See the ablation study in the following sections.)
2. Generated QA data is still not at the same level of the real data. (At least 4x+ more synthetic data is needed to reach the same level of accuracy.)

There are a lot of contents in this paper, and it can be a bit overwhelming. I wrote down parts of the paper that I think is most relevant in this post, and hopefully it can be helpful to you as well.

# Method

## Components

There are three or four stages in the data generation process. Each stage requires its own separate model:

0. [Optional] **Context generation**: When training the following three stage, half of the SQuAD 1.1 training data were used (Figure 2 below). But when testing/generating, we can choose to use real Wikipedia data or use a model to generate Wikipedia-like data.
1. **Answer Generation** ($\hat{p} \sim p(a|c)$): A BERT-style model to do answer extraction from the given context. The start and the end of the token span are jointly sampled.
2. **Question Generation** ($\hat{q} \sim p(q|\hat{a},c)$): Fine-tuned GPT-2 model to generation question from the context and the answer.
3. **Roundtrip Filtration** ($\hat{a} \stackrel{?}{=} a^{\ast} \sim p(a|c,\hat{q})$): A trained extractive QA model to get the answer from the context and the generated question. If the predicted answer matches the generated answer, we keep this triplet (context, answer, and question). Otherwise, the triplet is discarded.

The last step seems to be very strict. Any deviation from the generated answer will not be tolerated. However, given the EM(exact match) of the model trained on SQuAD 1.1 alone is already 87.7%, it's reasonable to expect that the quality of answer predicted by the filtration model to be quite accurate. The paper also propose an over-generation technique (generate two questions for each answer and context pair) to compensate those valid triplets being discarded.

{{< figure src="dataflow.png" caption="(Taken from the source paper)" >}}

## More Details

### Context Generation

Beside using Wikipedia articles as contexts, this paper alos generates completely synthetic contexts using a 8.3B GPT-2 model:

> This model was first trained with the Megatron-LM codebase for 400k iterations before being finetuned on only Wikipedia documents for 2k iterations. This allows us to generate high quality text from a distribution similar to Wikipedia by using top-p (p = 0.96) nucleus sampling.

### Answer Generation

This paper train the answer generation model to match the exact answer in the training data. This naturally ignores the other possible answers from the context, but seems to be a more generalizable way to do it.

The joint modeling of the starts and the ends of the answer span, which is reported to perform better, creates more candidates in the denominator in the calculation of the likelihood.

{{< figure src="answer_generation.png" caption="(Taken from the source paper)" >}}

I'm not very sure about the complexity and performance impact of this joint approach.

### Question Generation

This paper uses token type ids to identify the components in the triplets. The answer span in the context are also marked by the answer token id. Special tokens are also added to the start and the end of the questions.

{{< figure src="question_generation.png" caption="(Taken from the source paper)" >}}

### Number of Triplets Generated

As explained in the previous section, the paper use a over-generation technique to compensate the model precision problem. Two questions are generated for each answer and context pair (a.k.a. answer candidate). Answer candidates of the context are generated by top-k sampling within a nucleus of p = 0.9 (that means [we take the samples with highest likelihood until we either get K samples or the cumulative probabilities of the samples taken reaches 0.9](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277)).

{{< figure src="ablation-1.png" caption="(Taken from the source paper)" >}}

In the ablation study(which will be covered in the following sections), the models in stage 1 to 3 are trained with half of the SQuAD 1.1 training data, and other half is used to generate synthetic data. The performance of the QA model trained on the synthetic data is used to evaluate the quality of synthetic data.

From the table above (Table 4), we can see that the smaller model on average generated 2.75 valid triplets per context, and the larger model generated 4.36 triplets. Those synthetic datasets are already bigger than the SQuAD 1.1 training set.

# Experiments

## Model Scale

Table 4 (in the previous section) shows that larger models in stage 1 to 3 create better data for the downstream model, but it is not clear whether it was the quality of the data or the quantity of the data that helped.

{{< figure src="ablation-2.png" caption="(Taken from the source paper)" >}}

Table 5 shows that the quality of question generated does increase as the model scales up.
